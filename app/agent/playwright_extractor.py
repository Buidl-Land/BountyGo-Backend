"""
Playwright-based content extraction for handling anti-bot websites.
"""
import asyncio
import re
import urllib.parse
from datetime import datetime
from typing import Optional, Dict, Any
from playwright.async_api import async_playwright, Browser, BrowserContext, Page
from bs4 import BeautifulSoup
from readability.readability import Document

from .models import WebContent
from .exceptions import URLValidationError, ContentExtractionError


class PlaywrightContentExtractor:
    """åŸºäºPlaywrightçš„ç½‘é¡µå†…å®¹æå–å™¨ï¼Œå¯ä»¥å¤„ç†JavaScriptæ¸²æŸ“å’Œåçˆ¬æœºåˆ¶"""
    
    def __init__(self, 
                 timeout: int = 30,
                 max_content_length: int = 50000,
                 user_agent: str = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
                 headless: bool = True,
                 wait_for_selector: Optional[str] = None,
                 wait_time: int = 3):
        """
        åˆå§‹åŒ–Playwrightå†…å®¹æå–å™¨
        
        Args:
            timeout: é¡µé¢åŠ è½½è¶…æ—¶æ—¶é—´(ç§’)
            max_content_length: æœ€å¤§å†…å®¹é•¿åº¦
            user_agent: ç”¨æˆ·ä»£ç†å­—ç¬¦ä¸²
            headless: æ˜¯å¦ä½¿ç”¨æ— å¤´æ¨¡å¼
            wait_for_selector: ç­‰å¾…ç‰¹å®šé€‰æ‹©å™¨å‡ºç°
            wait_time: é¡µé¢åŠ è½½åç­‰å¾…æ—¶é—´(ç§’)
        """
        self.timeout = timeout * 1000  # Playwrightä½¿ç”¨æ¯«ç§’
        self.max_content_length = max_content_length
        self.user_agent = user_agent
        self.headless = headless
        self.wait_for_selector = wait_for_selector
        self.wait_time = wait_time * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        
        # å®‰å…¨çš„URLæ¨¡å¼
        self.allowed_schemes = {'http', 'https'}
        self.blocked_domains = {
            'localhost', '127.0.0.1', '0.0.0.0',
            '10.', '172.16.', '192.168.'  # å†…ç½‘åœ°å€å‰ç¼€
        }
        
        # æµè§ˆå™¨å®ä¾‹ç¼“å­˜
        self._browser: Optional[Browser] = None
        self._context: Optional[BrowserContext] = None
    
    def validate_url(self, url: str) -> str:
        """
        éªŒè¯URLçš„å®‰å…¨æ€§å’Œæœ‰æ•ˆæ€§
        
        Args:
            url: è¦éªŒè¯çš„URL
            
        Returns:
            str: æ ‡å‡†åŒ–çš„URL
            
        Raises:
            URLValidationError: URLéªŒè¯å¤±è´¥
        """
        try:
            # è§£æURL
            parsed = urllib.parse.urlparse(url)
            
            # æ£€æŸ¥åè®®
            if parsed.scheme not in self.allowed_schemes:
                raise URLValidationError(
                    f"ä¸æ”¯æŒçš„åè®®: {parsed.scheme}",
                    details=f"ä»…æ”¯æŒ: {', '.join(self.allowed_schemes)}"
                )
            
            # æ£€æŸ¥åŸŸå
            if not parsed.netloc:
                raise URLValidationError("æ— æ•ˆçš„URL: ç¼ºå°‘åŸŸå")
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºå†…ç½‘åœ°å€
            hostname = parsed.hostname
            if hostname:
                hostname_lower = hostname.lower()
                for blocked in self.blocked_domains:
                    if hostname_lower.startswith(blocked):
                        raise URLValidationError(
                            f"ä¸å…è®¸è®¿é—®å†…ç½‘åœ°å€: {hostname}",
                            details="å‡ºäºå®‰å…¨è€ƒè™‘ï¼Œç¦æ­¢è®¿é—®å†…ç½‘åœ°å€"
                        )
            
            # è¿”å›æ ‡å‡†åŒ–çš„URL
            return urllib.parse.urlunparse(parsed)
            
        except ValueError as e:
            raise URLValidationError(f"URLè§£æå¤±è´¥: {str(e)}")
    
    async def _get_browser_context(self) -> tuple[Browser, BrowserContext]:
        """è·å–æµè§ˆå™¨å’Œä¸Šä¸‹æ–‡å®ä¾‹"""
        if self._browser is None or self._context is None:
            playwright = await async_playwright().start()
            
            # å¯åŠ¨æµè§ˆå™¨ - å¢å¼ºåçˆ¬è™«èƒ½åŠ›
            self._browser = await playwright.chromium.launch(
                headless=self.headless,
                args=[
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                    '--disable-dev-shm-usage',
                    '--disable-accelerated-2d-canvas',
                    '--no-first-run',
                    '--no-zygote',
                    '--disable-gpu',
                    '--ignore-ssl-errors',
                    '--ignore-certificate-errors',
                    '--ignore-certificate-errors-spki-list',
                    '--disable-features=VizDisplayCompositor',
                    '--disable-background-timer-throttling',
                    '--disable-backgrounding-occluded-windows',
                    '--disable-renderer-backgrounding',
                    '--disable-web-security',
                    '--aggressive-cache-discard',
                    # å¢å¼ºåçˆ¬è™«æ£€æµ‹è§„é¿
                    '--disable-blink-features=AutomationControlled',
                    '--disable-features=VizDisplayCompositor,VizServiceDisplay',
                    '--disable-ipc-flooding-protection',
                    '--disable-extensions-file-access-check',
                    '--disable-extensions-http-throttling'
                ]
            )
            
            # åˆ›å»ºä¸Šä¸‹æ–‡ - å¢å¼ºåæ£€æµ‹
            self._context = await self._browser.new_context(
                user_agent=self.user_agent,
                viewport={'width': 1920, 'height': 1080},
                locale='zh-CN',
                timezone_id='Asia/Shanghai',
                # æ¨¡æ‹ŸçœŸå®æµè§ˆå™¨è¡Œä¸º
                extra_http_headers={
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
                    'Accept-Language': 'zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'Cache-Control': 'no-cache',
                    'Pragma': 'no-cache',
                    'Sec-Fetch-Dest': 'document',
                    'Sec-Fetch-Mode': 'navigate',
                    'Sec-Fetch-Site': 'none',
                    'Sec-Fetch-User': '?1',
                    'DNT': '1',
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1',
                },
                # ä¼ªè£…ç‰¹å¾
                ignore_https_errors=True,
                java_script_enabled=True,
                permissions=['geolocation', 'notifications']
            )
            
            # æ·»åŠ åæ£€æµ‹è„šæœ¬
            await self._context.add_init_script("""
                // ç§»é™¤webdriverç—•è¿¹
                Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
                
                // ä¼ªè£…chromeå¯¹è±¡
                window.chrome = { runtime: {} };
                
                // ä¼ªè£…plugins
                Object.defineProperty(navigator, 'plugins', {
                    get: () => [1, 2, 3, 4, 5]
                });
                
                // ä¼ªè£…languages
                Object.defineProperty(navigator, 'languages', {
                    get: () => ['zh-CN', 'zh', 'en']
                });
                
                // ç§»é™¤è‡ªåŠ¨åŒ–ç—•è¿¹
                delete window.cdc_adoQpoasnfa76pfcZLmcfl_Array;
                delete window.cdc_adoQpoasnfa76pfcZLmcfl_Promise;
                delete window.cdc_adoQpoasnfa76pfcZLmcfl_Symbol;
            """)
            
            # è®¾ç½®é»˜è®¤è¶…æ—¶
            self._context.set_default_timeout(self.timeout)
            self._context.set_default_navigation_timeout(self.timeout)
        
        return self._browser, self._context
    
    async def extract_content(self, url: str) -> WebContent:
        """
        ä½¿ç”¨Playwrightæå–ç½‘é¡µå†…å®¹
        
        Args:
            url: è¦æå–çš„URL
            
        Returns:
            WebContent: æå–çš„ç½‘é¡µå†…å®¹
            
        Raises:
            URLValidationError: URLéªŒè¯å¤±è´¥
            ContentExtractionError: å†…å®¹æå–å¤±è´¥
        """
        # éªŒè¯URL
        validated_url = self.validate_url(url)
        
        try:
            # è·å–æµè§ˆå™¨å®ä¾‹
            browser, context = await self._get_browser_context()
            
            # åˆ›å»ºæ–°é¡µé¢
            page = await context.new_page()
            
            try:
                # è®¾ç½®é¡µé¢äº‹ä»¶ç›‘å¬
                await self._setup_page_listeners(page)
                
                # å¯¼èˆªåˆ°é¡µé¢ - æ¸è¿›å¼ç­‰å¾…ç­–ç•¥
                print(f"ğŸŒ æ­£åœ¨è®¿é—®: {validated_url}")
                response = None
                
                # å°è¯•ä¸åŒçš„ç­‰å¾…ç­–ç•¥ï¼Œä»æœ€å®½æ¾å¼€å§‹
                wait_strategies = [
                    ('commit', 8000),      # åªç­‰å¾…å¯¼èˆªæäº¤
                    ('domcontentloaded', 12000),  # ç­‰å¾…DOMå†…å®¹
                    ('load', 20000)        # å®Œå…¨åŠ è½½ï¼ˆæœ€åå°è¯•ï¼‰
                ]
                
                for wait_until, timeout in wait_strategies:
                    try:
                        print(f"ğŸ“¡ å°è¯•ç­‰å¾…ç­–ç•¥: {wait_until} (è¶…æ—¶: {timeout}ms)")
                        response = await page.goto(validated_url, wait_until=wait_until, timeout=timeout)
                        print(f"âœ… å¯¼èˆªæˆåŠŸï¼Œä½¿ç”¨ç­–ç•¥: {wait_until}")
                        break
                    except Exception as e:
                        print(f"âš ï¸ ç­–ç•¥ {wait_until} å¤±è´¥: {str(e)[:100]}")
                        if wait_until == 'load':  # æœ€åä¸€ä¸ªç­–ç•¥ä¹Ÿå¤±è´¥äº†
                            raise
                
                if response is None:
                    raise ContentExtractionError("é¡µé¢å¯¼èˆªå¤±è´¥", details="æ— æ³•è®¿é—®æŒ‡å®šURL")
                
                # æ£€æŸ¥å“åº”çŠ¶æ€
                if response.status >= 400:
                    raise ContentExtractionError(
                        f"HTTPé”™è¯¯: {response.status}",
                        details=f"æœåŠ¡å™¨è¿”å›çŠ¶æ€ç : {response.status}"
                    )
                
                # ç­‰å¾…é¡µé¢åŠ è½½å®Œæˆ
                await self._wait_for_page_load(page)
                
                # è·å–é¡µé¢å†…å®¹
                html_content = await page.content()
                page_title = await page.title()
                final_url = page.url
                
                print(f"âœ… é¡µé¢åŠ è½½æˆåŠŸï¼Œå†…å®¹é•¿åº¦: {len(html_content)}")
                
                # æå–ç»“æ„åŒ–å†…å®¹
                extracted_content = await self._extract_structured_content(page)
                
                # ä½¿ç”¨readabilityæå–ä¸»è¦å†…å®¹
                doc = Document(html_content)
                title = doc.title() or page_title or "æ— æ ‡é¢˜"
                main_content = doc.summary()
                
                # æ¸…ç†å†…å®¹
                cleaned_content = self._clean_content(main_content)
                
                # å¦‚æœæœ‰ç»“æ„åŒ–å†…å®¹ï¼Œä¼˜å…ˆä½¿ç”¨
                if extracted_content and len(extracted_content) > len(cleaned_content) * 0.5:
                    final_content = extracted_content
                else:
                    final_content = cleaned_content
                
                # æå–metaä¿¡æ¯
                meta_description = await self._extract_meta_description(page)
                
                # é™åˆ¶æœ€ç»ˆå†…å®¹é•¿åº¦
                if len(final_content) > self.max_content_length:
                    final_content = final_content[:self.max_content_length] + "..."
                
                return WebContent(
                    url=final_url,
                    title=title.strip(),
                    content=final_content,
                    meta_description=meta_description,
                    extracted_at=datetime.utcnow()
                )
                
            finally:
                # å…³é—­é¡µé¢
                await page.close()
                
        except Exception as e:
            if isinstance(e, (URLValidationError, ContentExtractionError)):
                raise
            else:
                raise ContentExtractionError(
                    f"å†…å®¹æå–å¤±è´¥: {str(e)}",
                    details="ä½¿ç”¨Playwrightæå–å†…å®¹æ—¶å‘ç”Ÿé”™è¯¯"
                )
    
    async def _setup_page_listeners(self, page: Page):
        """è®¾ç½®é¡µé¢äº‹ä»¶ç›‘å¬å™¨"""
        try:
            # å±è”½ä¸å¿…è¦çš„èµ„æºç±»å‹ï¼Œæé«˜åŠ è½½é€Ÿåº¦
            await page.route("**/*.{png,jpg,jpeg,gif,svg,ico,woff,woff2,ttf,eot}", lambda route: route.abort())
            await page.route("**/*analytics*", lambda route: route.abort())
            await page.route("**/*tracking*", lambda route: route.abort())
            await page.route("**/*ads*", lambda route: route.abort())
            
            # ç›‘å¬æ§åˆ¶å°æ¶ˆæ¯ï¼ˆä»…è®°å½•ä¸¥é‡é”™è¯¯ï¼‰
            page.on("console", lambda msg: None if msg.type in ["log", "info", "warning"] 
                   else print(f"Console: {msg.text}"))
            
            # ç›‘å¬é¡µé¢é”™è¯¯ï¼ˆä»…è®°å½•å…³é”®é”™è¯¯ï¼‰
            page.on("pageerror", lambda error: print(f"Page error: {error}") 
                   if "ERR_SSL" not in str(error) and "ERR_FAILED" not in str(error) else None)
                   
        except Exception as e:
            print(f"âš ï¸ é¡µé¢ç›‘å¬å™¨è®¾ç½®å¤±è´¥: {e}")

    async def _wait_for_page_load(self, page: Page):
        """æ™ºèƒ½ç­‰å¾…é¡µé¢åŠ è½½ - ä¼˜åŒ–ç‰ˆæœ¬"""
        try:
            # å…ˆç­‰å¾…DOMå†…å®¹åŠ è½½å®Œæˆ
            await page.wait_for_load_state('domcontentloaded', timeout=10000)
            
            # ç­‰å¾…ä¸€å°æ®µæ—¶é—´è®©åŠ¨æ€å†…å®¹æ¸²æŸ“
            await page.wait_for_timeout(2000)
            
            # å°è¯•ç­‰å¾…ç½‘ç»œç©ºé—²ï¼Œä½†ä½¿ç”¨è¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´
            try:
                await page.wait_for_load_state('networkidle', timeout=5000)
            except:
                # å¦‚æœç½‘ç»œç©ºé—²ç­‰å¾…è¶…æ—¶ï¼Œç»§ç»­æ‰§è¡Œï¼ˆå¾ˆå¤šç½‘ç«™æœ‰æŒç»­çš„åå°è¯·æ±‚ï¼‰
                print("ğŸ”„ ç½‘ç»œä»æœ‰æ´»åŠ¨ï¼Œä½†ç»§ç»­æå–å†…å®¹...")
            
            # å¦‚æœæŒ‡å®šäº†é€‰æ‹©å™¨ï¼Œç­‰å¾…å…¶å‡ºç°
            if self.wait_for_selector:
                try:
                    await page.wait_for_selector(self.wait_for_selector, timeout=5000)
                except:
                    print(f"âš ï¸ ç­‰å¾…é€‰æ‹©å™¨è¶…æ—¶: {self.wait_for_selector}")
            
            print("âœ… é¡µé¢å†…å®¹å·²å‡†å¤‡å°±ç»ª")
                
        except Exception as e:
            print(f"âš ï¸ é¡µé¢åŠ è½½ç­‰å¾…å¼‚å¸¸: {e}")
            # ç»§ç»­æ‰§è¡Œï¼Œä¸æŠ›å‡ºå¼‚å¸¸
    
    async def _extract_structured_content(self, page: Page) -> str:
        """æå–ç»“æ„åŒ–å†…å®¹"""
        try:
            # å°è¯•æå–ä¸»è¦å†…å®¹åŒºåŸŸ
            content_selectors = [
                'main',
                'article',
                '[role="main"]',
                '.content',
                '.main-content',
                '.post-content',
                '.entry-content',
                '#content',
                '#main-content'
            ]
            
            for selector in content_selectors:
                try:
                    element = await page.query_selector(selector)
                    if element:
                        text_content = await element.inner_text()
                        if text_content and len(text_content.strip()) > 100:
                            print(f"âœ… ä½¿ç”¨é€‰æ‹©å™¨æå–å†…å®¹: {selector}")
                            return text_content.strip()
                except:
                    continue
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸»è¦å†…å®¹åŒºåŸŸï¼Œæå–bodyå†…å®¹
            body_element = await page.query_selector('body')
            if body_element:
                body_text = await body_element.inner_text()
                return body_text.strip() if body_text else ""
            
            return ""
            
        except Exception as e:
            print(f"âš ï¸ ç»“æ„åŒ–å†…å®¹æå–å¤±è´¥: {e}")
            return ""
    
    async def _extract_meta_description(self, page: Page) -> Optional[str]:
        """æå–metaæè¿°ä¿¡æ¯"""
        try:
            # å°è¯•å¤šç§metaæè¿°é€‰æ‹©å™¨
            meta_selectors = [
                'meta[name="description"]',
                'meta[property="og:description"]',
                'meta[name="twitter:description"]'
            ]
            
            for selector in meta_selectors:
                element = await page.query_selector(selector)
                if element:
                    content = await element.get_attribute('content')
                    if content and content.strip():
                        return content.strip()
            
            return None
            
        except Exception:
            return None
    
    def _clean_content(self, html_content: str) -> str:
        """æ¸…ç†HTMLå†…å®¹"""
        try:
            # ä½¿ç”¨BeautifulSoupè§£æHTML
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ç§»é™¤ä¸éœ€è¦çš„æ ‡ç­¾
            unwanted_tags = [
                'script', 'style', 'nav', 'header', 'footer', 'aside',
                'advertisement', 'ads', 'iframe', 'embed', 'object',
                'form', 'input', 'button', 'select', 'textarea',
                'noscript', 'meta', 'link', 'base'
            ]
            for tag in soup(unwanted_tags):
                tag.decompose()
            
            # è·å–æ–‡æœ¬å†…å®¹
            text = soup.get_text(separator=' ', strip=True)
            
            # æ¸…ç†æ–‡æœ¬
            text = re.sub(r'\s+', ' ', text)
            text = re.sub(r'\n\s*\n', '\n\n', text)
            
            return text.strip()
            
        except Exception:
            # å¦‚æœæ¸…ç†å¤±è´¥ï¼Œè¿”å›åŸå§‹å†…å®¹
            return html_content
    
    async def close(self):
        """å…³é—­æµè§ˆå™¨å®ä¾‹"""
        if self._context:
            await self._context.close()
            self._context = None
        
        if self._browser:
            await self._browser.close()
            self._browser = None
    
    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        await self.close()


class SmartContentExtractor:
    """æ™ºèƒ½å†…å®¹æå–å™¨ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ä½³æå–æ–¹å¼"""
    
    def __init__(self, 
                 timeout: int = 30,
                 max_content_length: int = 50000,
                 prefer_playwright: bool = True):
        """
        åˆå§‹åŒ–æ™ºèƒ½å†…å®¹æå–å™¨
        
        Args:
            timeout: è¶…æ—¶æ—¶é—´
            max_content_length: æœ€å¤§å†…å®¹é•¿åº¦
            prefer_playwright: æ˜¯å¦ä¼˜å…ˆä½¿ç”¨Playwright
        """
        self.timeout = timeout
        self.max_content_length = max_content_length
        self.prefer_playwright = prefer_playwright
        
        # éœ€è¦ä½¿ç”¨Playwrightçš„åŸŸåæ¨¡å¼
        self.playwright_domains = {
            # ä»£ç æ‰˜ç®¡å¹³å°
            'github.com',
            'gitlab.com',
            'bitbucket.org',
            
            # å¼€å‘è€…ç¤¾åŒº
            'stackoverflow.com',
            'stackexchange.com',
            'dev.to',
            'hackernoon.com',
            'hashnode.com',
            
            # åšå®¢å¹³å°
            'medium.com',
            'substack.com',
            'notion.so',
            
            # ä»»åŠ¡/é¡¹ç›®å¹³å°
            'dorahacks.io',
            'gitcoin.co',
            'upwork.com',
            'freelancer.com',
            'fiverr.com',
            '99designs.com',
            
            # ç¤¾äº¤åª’ä½“ (åçˆ¬è™«ä¸¥æ ¼)
            'twitter.com',
            'x.com',
            'facebook.com',
            'linkedin.com',
            'instagram.com',
            'reddit.com',
            'discord.com',
            
            # å…¶ä»–å¤æ‚JavaScriptç½‘ç«™
            'notion.site',
            'airtable.com',
            'figma.com',
            'canva.com',
            'trello.com'
        }
    
    def _should_use_playwright(self, url: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨Playwright"""
        if not self.prefer_playwright:
            return False
        
        try:
            parsed = urllib.parse.urlparse(url)
            domain = parsed.netloc.lower()
            
            # æ£€æŸ¥æ˜¯å¦åœ¨éœ€è¦Playwrightçš„åŸŸååˆ—è¡¨ä¸­
            for playwright_domain in self.playwright_domains:
                if playwright_domain in domain:
                    return True
            
            return False
            
        except:
            return False
    
    async def extract_content(self, url: str) -> WebContent:
        """
        æ™ºèƒ½æå–å†…å®¹
        
        Args:
            url: è¦æå–çš„URL
            
        Returns:
            WebContent: æå–çš„ç½‘é¡µå†…å®¹
        """
        # åˆ¤æ–­ä½¿ç”¨å“ªç§æå–æ–¹å¼
        use_playwright = self._should_use_playwright(url)
        
        if use_playwright:
            print("ğŸ­ ä½¿ç”¨Playwrightæå–å†…å®¹")
            async with PlaywrightContentExtractor(
                timeout=self.timeout,
                max_content_length=self.max_content_length
            ) as extractor:
                return await extractor.extract_content(url)
        else:
            print("ğŸŒ ä½¿ç”¨ä¼ ç»ŸHTTPæ–¹å¼æå–å†…å®¹")
            # å¯¼å…¥åŸå§‹çš„å†…å®¹æå–å™¨
            from .content_extractor import ContentExtractor
            extractor = ContentExtractor(
                timeout=self.timeout,
                max_content_length=self.max_content_length
            )
            return await extractor.extract_content(url)